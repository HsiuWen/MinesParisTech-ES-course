{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Multiple layer Neural Network (MLP) \n",
    "This notebook has been prepared by Hsiu-Wen (Kelly) Chang from MINES ParisTech for the class machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch \n",
    "\n",
    "Pytorch is the most powerful open source deep learning framework. It is a replacement for NumPy (a python library for scientific computing) to use the power of GPUs and other accelerators. Also, the automatic differentiation library is useful to implement neural netorks. In this practical lession, you will first review the use of Numpy to solve a regression problem. Then the use of Pytorch to solve the same problem. Finally, you will practice how to create a complete class to solve a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Part 1: Use Numpy to build a simple neural network fit a third order polynomial to sine function and how to replace it with Pytorch\n",
    "\n",
    "Part 2: Practice the use of neural network from sratch: architecture, activations\n",
    "\n",
    "Part 3: Practice the use of pytorch module to train an image classifier\n",
    "\n",
    "*** You have four practice questions for your own coding ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Numpy and Pytorch (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-6\n",
    "epochs = 2000\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3``\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 200 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2*(y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "# Print final result\n",
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
    "plt.plot(x, y_pred, label='Predicted')\n",
    "plt.plot(x, y, label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this easy code, we need to compute gradients and manually update weights, and we are not using a GPU to accelerate the calculation. We are now starting to use PyTorch and a GPU. \n",
    "\n",
    "### Tensor and autograd \n",
    "- tensor is an n-dimensional array that can keep track of a computational graph and gradients, which is important for deep learning. When ``x.requires_grad`` is true, PyTorch's autograd will track its gradients during forward pass. \n",
    "- autograd is a package that define a compuational graph while execute forward pass of a NN. Nodes in the graphe will be tensors and functions will be edges. Backprogating through this graph then allows us to easily compute gradients \n",
    "\n",
    "You should be cautious about it because you don't need it on when it is during test mode. And a common error is caused by not reset gradient after the weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# move input and output to the device\n",
    "x = torch.tensor(x, device=device, dtype=dtype)\n",
    "y = torch.tensor(y, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    #use autograd to compute backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "# Final prediction and plotting\n",
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move to modern way to solve this simple task by Pytorch. \n",
    "\n",
    " - ``nn`` module\n",
    "\n",
    "Computaional graphs for deeper network need high-level of abstraction: layers. The ``nn`` package defines a set of Modules, which are roughly equivalent to a neural network layers. Each module receives input Tensors and computes output Tensors, but also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions. \n",
    "\n",
    " - ``optim``\n",
    "\n",
    "We can see that update weights of our models by manually mutating the tensors holding learnable parameters (some might not be learnable when we do transfer learning) with torch.no_grad(). The ``optim`` package provides us modern optimization algorithms such as AdaGrad, RMSProp, Adam. Here we demonstrate RMSprop algorithm. \n",
    "\n",
    " - fit function\n",
    "This function reduce the repeated code for training. This is useful when you fine tuing hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continaul with the same example\n",
    "# prepare input features\n",
    "x_features = torch.stack((x, x**2, x**3), dim=1)  # shape (2000, 3)\n",
    "\n",
    "#define model, by default, the newly-created module lives on CPU. It is needed to make sure the all the tensors and modules are on the same device\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0,1) # it flatterns the output of the linear layer to 1D tensor to match the shape of y\n",
    ")\n",
    "model.to(device)\n",
    "#define loss function: we use mean squared error for regression\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# define optimizer: we use RMSprop\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = model(x_features)  # shape (2000,)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 200 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "linear_layer=model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Wrape all you need to a class\n",
    "\n",
    "In deep learning, it is important to test several models with different hyperparameter settings. Here is how we can wrap everything we need to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpregression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init()\n",
    "        self.a=torch.nn.Parameter(torch.randn())\n",
    "        self.b=torch.nn.Parameter(torch.randn())\n",
    "        self.c=torch.nn.Parameter(torch.randn())\n",
    "        self.d=torch.nn.Parameter(torch.randn())\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Practice 1}$: use this class to redo the training of the regression task. Overall, you should have the flowchat of the algorithm like this\n",
    "1. initialization\n",
    "2. forward\n",
    "3. loss\n",
    "4. Backward\n",
    "5. optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Pytorch on classifiction with hand-drawn digits images\n",
    "----------------\n",
    "\n",
    "We are going to targeting the application of predicting the hand-drawn digits (between 0 and 9) in a given black-and-white image. First, we will see how to preprocess the data and eventually use the public dataset provided by ```torchvision```. \n",
    "\n",
    "(1) download raw images by URL. \n",
    "![](http://perso.mines-paristech.fr/fabien.moutarde/ES_MachineLearning/TP_convNets/mnist.png)  \n",
    "You can use [pathlib](https://docs.python.org/3/library/pathlib.html)\n",
    "for dealing with paths (part of the Python 3 standard library), and download the dataset using\n",
    "[requests](http://docs.python-requests.org/en/master/). We will only import modules when we use them, so you can see exactly what's being used at each point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print((PATH / FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data. Each image is 28x28 pixels, and is being stored as a flattened rwo of length 784. To have a look at one provided image, we need to reshape it to 2D. In addition, PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to convert our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "plt.imshow(x_train[0].reshape(28,28), cmap='gray')\n",
    "print(f\"size of training data (number of samples, number of pixels): {x_train.shape}\")\n",
    "print(f\"size of test data (number of samples, number of pixels): {x_valid.shape}\")\n",
    "\n",
    "# Convert data to torch tensors and move to detected device\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    lambda x: torch.tensor(x).to(device), (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a model with only one layer using nothing but PyTorch tensor operations. \n",
    "Follow these steps to create the corresponding functions\n",
    "\n",
    "Then we will combine all of them as a class for training\n",
    "\n",
    "(1) Use tensors to store information: x = torch.tensor(x) or any built-in functions of torch \n",
    "\n",
    "(2) Initialize weights with randomness\n",
    "\n",
    "(3) use ``requires_grad`` to tell the computer that they require a gradient. This means that Pytorch will record all of the operations done on the tensor and calculate the gradient during back-propagation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NN with linear model and (28*28) inputs and 10 outputs\n",
    "\n",
    "# Initialize weights and bias \n",
    "weights = torch.randn(28*28, 10, device=device, dtype=dtype, requires_grad=True)\n",
    "bias = torch.randn(10, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has several built-in activation functions in ```torch.nn.functional```, however, it is necessary to know how to define one by yourself. PyTorch will\n",
    "even create fast GPU or vectorized CPU code for your function automatically. Let's define log_softmax function and use it as our activation to increase the complexity of our NN\n",
    "\n",
    "$${\\bf \\theta} = \\log\\left(\\frac{{\\exp}({x_{i})}}{\\sum_{j}{\\bf \\exp}({x_{j})}}\\right)$$\n",
    "$${\\bf \\theta} = x-\\log{\\sum_{j}{\\bf \\exp}({x_{j})}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    # Implement log_softmax function\n",
    "    return x - torch.logsumexp(x, dim=1, keepdim=True)\n",
    "\n",
    "def model(x):\n",
    "    # Implement the NN model\n",
    "    return log_softmax(x.mm(weights) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the ``@`` stands for the dot product operation.\n",
    "\n",
    "Now we design a batch size of 64 to train each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64  # batch size\n",
    "\n",
    "xb = x_train[0:bs]  # a mini-batch from x without randomness\n",
    "\n",
    "preds = model(xb)  # predictions\n",
    "\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the ``preds`` tensor contains not only the tensor values, but also a\n",
    "gradient function. We'll use this later to do backprop.\n",
    "\n",
    "Let's implement negative log-likelihood to use as the loss function\n",
    "(again, we can just use standard Python):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our loss with our random model, so we can see if we improve\n",
    "after a backprop pass later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(preds[range(yb.shape[0]),yb])\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also implement a function to calculate the accuracy of our model.\n",
    "For each prediction, if the index with the largest value matches the\n",
    "target value, then the prediction was correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy of our random model, so we can see if our\n",
    "accuracy improves as our loss improves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop.  For each iteration, we will:\n",
    "\n",
    "- select a mini-batch of data (of size ``bs``)\n",
    "- use the model to make predictions\n",
    "- calculate the loss\n",
    "- ``loss.backward()`` updates the gradients of the model, in this case, ``weights``\n",
    "  and ``bias``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5  # learning rate\n",
    "epochs = 2  # how many epochs to train for\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((x_train.shape[0] - 1) // bs + 1): #floor division\n",
    "        start_i=i*bs\n",
    "        end_i =start_i + bs\n",
    "        # grab batch of data (xb) and targets (yb)\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        # predict output\n",
    "        preds = model(xb)\n",
    "    \n",
    "        # calculate loss\n",
    "        loss = loss_func(preds, yb)\n",
    "\n",
    "        # backpropagation error\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            #update weights and bias\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            # zero the gradients\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "\n",
    "    print('Epochs {}: loss {}, accuracy {}'.format(epoch, loss_func(model(xb), yb), accuracy(model(xb), yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it: we've created and trained a minimal neural network (in this case, a\n",
    "logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let's check the loss and accuracy and compare those to what we got\n",
    "earlier. We expect that the loss will have decreased and accuracy to\n",
    "have increased, and they have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss:', loss_func(model(xb), yb).item(), 'accuracy:', accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: package your model for training, testing and validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------\n",
    "The above work can be easily done by using torch.nn.functional. Please rewrite the above code to be more concise and flexible. Make it shorter, more understandable.\n",
    "\n",
    "Here is the list of modules that you should try to read:\n",
    "1. [torch.nn](https://pytorch.org/docs/stable/nn.html)\n",
    "2. [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "3. [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)\n",
    "4. [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Step 1: use activation and loss function from ``torch.nn.functional``. We use torch.nn.Parameter for defining learnable parameters of a model. They can be returned by Minist_Logistic.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class Mnist_Logistic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.fc1(x), dim=1)\n",
    "\n",
    "# Intialize the model and move it to the device\n",
    "model = Mnist_Logistic().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: Define a loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# see the size of learnable parameters\n",
    "params=list(model.parameters())\n",
    "print(params[0].shape)  # weights\n",
    "print(params[1].shape)  # bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See your gradients are correctly computed and updated in the training loop\n",
    "out=model(x_train[0].unsqueeze(0))  # unsqueeze to add batch dimension\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "loss=criterion(out, y_train[0].unsqueeze(0))\n",
    "\n",
    "print(loss)\n",
    "# For illustration, let us follow a few steps backward\n",
    "print(loss.grad_fn)  # NLLLoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # LogSoftmax\n",
    "\n",
    "model.zero_grad()\n",
    "print('fc1 weight grad before backward:', model.fc1.weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('fc1 weight grad after backward:', model.fc1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to run the training loops with many lines. Here is the good way to make your code clean and short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train, epochs=5, lr=0.1):\n",
    "    # define loss function and optimizer inside the fit function\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((x_train.shape[0] - 1) // bs + 1): #floor division\n",
    "            start_i=i*bs\n",
    "            end_i =start_i + bs\n",
    "            # grab batch of data (xb) and targets (yb)\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            # predict output\n",
    "            preds = model(xb)\n",
    "        \n",
    "            # calculate loss\n",
    "            loss = loss_func(preds, yb)\n",
    "\n",
    "            # backpropagation error and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epochs {}: loss {}, accuracy {}'.format(epoch, loss_func(model(xb), yb), accuracy(model(xb), yb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate your function and check loss value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(x_train, y_train, epochs=3, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Practice 2}$: Improve accuracy by tuning the hyperparameters or the NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using TensorDataset\n",
    "\n",
    "PyTorch has an abstract Dataset class. A Dataset can be anything that has a __len__ function (called by Python's standard len function) and a __getitem__ function as a way of indexing into it. This [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) walks through a nice example of creating a custom FacialLandmarkDataset class as a subclass of Dataset.\n",
    "\n",
    "By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "\n",
    "# This is how to get a batch of data from the TensorDataset\n",
    "xb, yb = train_ds[0:bs]\n",
    "\n",
    "print(xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using DataLoader\n",
    "\n",
    "Pytorch's ``DataLoader`` is responsible for managing batches. You can\n",
    "create a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier\n",
    "to iterate over batches. Rather than having to use ``train_ds[i*bs : i*bs+bs]``,\n",
    "the DataLoader gives us each minibatch automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the DataLoader to get a batch of data\n",
    "xb, yb = next(iter(train_dl)) \n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\n",
    "our training loop is now dramatically smaller and easier to understand. Let's\n",
    "now try to add the basic features necessary to create effecive models in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  Add validation\n",
    "In reality, we always need to take care the overfitting issue. We can avoid it by:\n",
    "(1) add validation\n",
    "(2) shuffling the training data prevent correlation between batches and overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that we always call ``model.train()`` before training, and ``model.eval()``\n",
    "before inference, because these are used by layers such as ``nn.BatchNorm2d``\n",
    "and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Practice 3}$: rewrite the fit function with train_dl function and rint the validation loss at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Practice 4}$: plot loss values and validations values\n",
    "In order to understand the progress of your training, error curves with test and validation are very common to be shown.\n",
    "Change your code in order to see the curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary \n",
    "-----------------\n",
    "\n",
    "We have seen and practice the following functions\n",
    "\n",
    " - **torch.nn**\n",
    "\n",
    "   + ``Module``: creates a callable which behaves like a function, but can also\n",
    "     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n",
    "     contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n",
    "     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n",
    "   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n",
    "     which contains activation functions, loss functions, etc, as well as non-stateful\n",
    "     versions of layers such as convolutional and linear layers.\n",
    " - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n",
    "   of ``Parameter`` during the backward step\n",
    " - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n",
    "   including classes provided with Pytorch such as ``TensorDataset``\n",
    " - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
