{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7WBO0STnTG2"
   },
   "source": [
    "# Lab3 Implementation of simple RNN and LSTM \n",
    "\n",
    "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\n",
    "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\n",
    "\n",
    "In this lab, we are going to practice \n",
    "\n",
    "1. many-to-one by RNN: given several words, predict the next word\n",
    "2. many-to-one by LSTM: given several letters, predict the final letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ohpiJoXn9nT"
   },
   "source": [
    "## 1. Many-to-one by RNN (word level): Predict what is the next word\n",
    "\n",
    "Our task today is to predict the next word by given several words before. For example, we expect to have answer to be 'cat' when user key in 'I like'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4A_SAJUkz9T"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHwH300KolLb"
   },
   "source": [
    "### 1.1 Data preparation\n",
    "\n",
    "Here are three sentences and each of them has three words. We are going to use it as training sample. The design is to feed first two words and let the machine find the final word. However, the computer can't do mathematic operations on characters. Therefore, the first step is to encode the input to digital numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URZTfCdrodnO",
    "outputId": "7bfe2f84-20f2-4804-ad00-95c7f32ea555"
   },
   "outputs": [],
   "source": [
    "# Create the input data, you are welcome to add the words you like\n",
    "sentences = [ \"i like cat\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "# Define all the possible words\n",
    "word_list = \" \".join(sentences).split()\n",
    "\n",
    "word_list = list(set(word_list))\n",
    "\n",
    "# dictionary that chanage the given word to number. {love: 0, hate:1,...}\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# dictionary that chanage the number to word. {0: love, 1: hate,...}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "# number of class(=number of vocab)\n",
    "n_class = len(word_dict)\n",
    "\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENif1UQqE5Vi"
   },
   "source": [
    "### 1.2 Data preprocessing\n",
    "\n",
    "Define batch function to let machine know how he should use it during training.\n",
    "Here we give all the data we have for simplication. But in real case, you should not do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lSpSxU2D5r4"
   },
   "outputs": [],
   "source": [
    "# Function to encode the sentence into a vector \n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ5-1MR4FFiF",
    "outputId": "5669b385-d1a8-408f-a4f9-58aecc9bdc92"
   },
   "outputs": [],
   "source": [
    "# to Torch.Tensor\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = Variable(torch.Tensor(input_batch))\n",
    "target_batch = Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "print('Dimension of input_patch:', input_batch.shape)\n",
    "print(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ80UekGDmtw"
   },
   "source": [
    "### 1.3 Network\n",
    "\n",
    "Torch.nn provide a function call nn.RNN which is a multi-layer Elman RNN with $tanh$ or $ReLU$ (controlled by nonlinearity parameter) to an input sequence.\n",
    "\n",
    "The equation to compute the hidden state is $$h_t=tanh(W_{ih}x_t+b_{ih}+w_{hh}h_{t-1}+b_{hh}) $$\n",
    "\n",
    "Further information about how you can use it, check this [link](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMk0ffFHGc8v"
   },
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self,n_class=7, n_hidden=5):\n",
    "        super(TextRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        outputs = outputs[-1] # [batch_size, num_directions(=1) * n_hidden]\n",
    "        model = torch.mm(outputs, self.W) + self.b # model : [batch_size, n_class]\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKdx4ZGduZ19"
   },
   "outputs": [],
   "source": [
    "# Paramters for the network\n",
    "batch_size = len(sentences)\n",
    "n_step = 2 # number of cells(= number of Step)\n",
    "n_hidden = 5 # number of hidden units in one cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGqRO_R0P5aD"
   },
   "outputs": [],
   "source": [
    "model = TextRNN(n_class, n_hidden)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZACZzfaVQaZl"
   },
   "source": [
    "Lets see how this model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyC3Un5kP8vN",
    "outputId": "ddbdbb64-6d7d-45fb-89ea-150d5f34ff39"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usnLJmJhSyV9"
   },
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJ2sYct1QJqF",
    "outputId": "a0a6f249-8a97-4494-d9d5-f3faf6b4eff4"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    # Reset the gradient buffer \n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    # hidden : [num_layers * num_directions, batch, hidden_size]\n",
    "    hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n",
    "    # input_batch : [batch_size, n_step, n_class]\n",
    "    output = model(hidden, input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_N2M5rSTcNj"
   },
   "source": [
    "### 1.5 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9StYmSPjTP7N",
    "outputId": "1454837b-05bd-44f3-a958-f42626d05eaa"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "# Initial hidden state 0\n",
    "hidden = Variable(torch.zeros(1, batch_size, n_hidden))\n",
    "\n",
    "print('Raw output of this model:\\n',model(hidden, input_batch))\n",
    "\n",
    "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29mr61tFVEmX"
   },
   "source": [
    "### Task 1: create the sevearl french words and the corresponding english words by yourself. Train a RNN model that can translate the french word into english word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSBZJqpRTiY7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjsQaSOkVk8i"
   },
   "source": [
    "## 2. Many-to-one LSTM (character level): Predict what is the next letter\n",
    "\n",
    "In this task, we will give our network to predict the final letter for us uisng LSTM. For example, if we key in 'lov' then the machine should give us 'e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo24zXN1d131"
   },
   "source": [
    "### 2.1 Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1BkuKk-db_c"
   },
   "outputs": [],
   "source": [
    "# we need define all the possible letters\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "#word dictionary that can use to get the corresponding encoded number\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# number dictionary that can be used to get the corresponding letter\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "\n",
    "n_class = len(word_dict) # number of class(=number of vocab)\n",
    "\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTNi04tpeCNJ"
   },
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n",
    "        target = word_dict[seq[-1]] # 'e' is target\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyze2ep0eOWj"
   },
   "source": [
    "### 2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RV0SyyCeKaE"
   },
   "outputs": [],
   "source": [
    "# TextLSTM Parameters\n",
    "n_step = 3\n",
    "n_hidden = 128\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\n",
    "\n",
    "        hidden_state = Variable(torch.zeros(1, len(X), n_hidden))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        cell_state = Variable(torch.zeros(1, len(X), n_hidden))     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "\n",
    "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs[-1]  # [batch_size, n_hidden]\n",
    "        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoQyxb9RehCj"
   },
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9wn3ZP-eonH",
    "outputId": "81ebce9b-8633-4990-bd64-a5e8901ad5a3"
   },
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "model = TextLSTM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "output = model(input_batch)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHlCD3aiez7d"
   },
   "source": [
    "### 2.4 Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pj_ODzDxev9Q",
    "outputId": "2748de06-871f-48fc-830c-285c0b196213"
   },
   "outputs": [],
   "source": [
    "inputs = [sen[:3] for sen in seq_data]\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8df7rkUfXfR"
   },
   "source": [
    "### Task 2: \n",
    "\n",
    "1. Use whatever way you like, add more than 20 vocabulary and reuse the code to do the same task\n",
    "1. modify the model to make it predict one word each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-ZJhmSfe5-X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaLAB5CV-biG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You should think about the problem when we have much bigger vocabulary that using dict to enumerate the words will make it very inefficient.\n",
    "\"Embedding\" and \"Tokenizer\" are the two soltuions available in [Keras](https://keras.io/examples/nlp/). You should take a look at this document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5Gq-iEA-9wC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmPW7hN5DhpL"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(7, 3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNcjO9odI67yqCTVsVW6In",
   "collapsed_sections": [],
   "name": "Lab4_RNN_LSTM_language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
