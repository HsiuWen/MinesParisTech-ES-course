{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7WBO0STnTG2"
   },
   "source": [
    "# Lab3 Implementation of simple RNN and LSTM \n",
    "\n",
    "This notebook has been prepared by Hsiu-Wen Chang from MINES ParisTech\n",
    "Shall you have any problem, send me [email](hsiu-wen.chang_joly@mines-paristech.fr)\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. many-to-one by RNN: given several words, predict the next word\n",
    "2. many-to-one by LSTM: given several letters, predict the final letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ohpiJoXn9nT"
   },
   "source": [
    "### 1. Many-to-one by RNN (word level): Predict what is the next word\n",
    "\n",
    "Our task today is to predict the next word by given several words before. For example, we expect to have answer to be 'cat' when user key in 'I like'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4A_SAJUkz9T"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHwH300KolLb"
   },
   "source": [
    "#### 1.1 Data preparation\n",
    "\n",
    "Here are three sentences and each of them has three words. We are going to use it as training sample. The design is to feed first two words and let the machine find the final word. However, the computer can't do mathematic operations on characters. Therefore, the first step is to encode the input to digital numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URZTfCdrodnO",
    "outputId": "7bfe2f84-20f2-4804-ad00-95c7f32ea555"
   },
   "outputs": [],
   "source": [
    "# Create the input data, you are welcome to add the words you like\n",
    "sentences = [ \"i like cat\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "# Define all the possible words\n",
    "word_list = \" \".join(sentences).split()\n",
    "\n",
    "word_list = list(set(word_list))\n",
    "\n",
    "# dictionary that chanage the given word to number. {love: 0, hate:1,...}\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "\n",
    "# dictionary that chanage the number to word. {0: love, 1: hate,...}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "# number of class(=number of vocab)\n",
    "n_class = len(word_dict)\n",
    "\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENif1UQqE5Vi"
   },
   "source": [
    "#### 1.2 Data preprocessing\n",
    "\n",
    "Define batch function to let machine know how he should use it during training.\n",
    "Here we give all the data we have for simplication. But in real case, you should not do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lSpSxU2D5r4"
   },
   "outputs": [],
   "source": [
    "# Function to encode the sentence into a vector \n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split()\n",
    "        input = [word_dict[n] for n in word[:-1]]\n",
    "        target = word_dict[word[-1]]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(np.eye(n_class)[target])\n",
    "\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ5-1MR4FFiF",
    "outputId": "5669b385-d1a8-408f-a4f9-58aecc9bdc92"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "# move input and output to the device\n",
    "input_batch = torch.tensor(input_batch, device=device, dtype=dtype)\n",
    "target_batch = torch.tensor(target_batch, device=device, dtype=dtype)\n",
    "\n",
    "print('Dimension of input_patch:', input_batch.shape)\n",
    "print(input_batch)\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ80UekGDmtw"
   },
   "source": [
    "#### 1.3 Network\n",
    "\n",
    "Torch.nn provide a function call nn.RNN which is a multi-layer Elman RNN with $tanh$ or $ReLU$ (controlled by nonlinearity parameter) to an input sequence.\n",
    "\n",
    "The equation to compute the hidden state is $$h_t=tanh(W_{ih}x_t+b_{ih}+w_{hh}h_{t-1}+b_{hh}) $$\n",
    "\n",
    "Further information about how you can use it, check this [link](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMk0ffFHGc8v"
   },
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self,n_class=7, n_hidden=5):\n",
    "        super(TextRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n",
    "        outputs, hidden = self.rnn(X, hidden)\n",
    "        # outputs : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        outputs = outputs[-1] # [batch_size, num_directions(=1) * n_hidden]\n",
    "        model = torch.mm(outputs, self.W) + self.b # model : [batch_size, n_class]\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKdx4ZGduZ19"
   },
   "outputs": [],
   "source": [
    "# Paramters for the network\n",
    "batch_size = len(sentences)\n",
    "n_step = 2 # number of cells(= number of Step)\n",
    "n_hidden = 5 # number of hidden units in one cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGqRO_R0P5aD"
   },
   "outputs": [],
   "source": [
    "# Intialize the model and move it to the device\n",
    "model = TextRNN(n_class, n_hidden).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZACZzfaVQaZl"
   },
   "source": [
    "Lets see how this model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyC3Un5kP8vN",
    "outputId": "ddbdbb64-6d7d-45fb-89ea-150d5f34ff39"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usnLJmJhSyV9"
   },
   "source": [
    "#### 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJ2sYct1QJqF",
    "outputId": "a0a6f249-8a97-4494-d9d5-f3faf6b4eff4"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(5000):\n",
    "    # Reset the gradient buffer \n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    # hidden : [num_layers * num_directions, batch, hidden_size]\n",
    "    hidden = torch.tensor(torch.zeros(1, batch_size, n_hidden),device=device, dtype=dtype)\n",
    "    # input_batch : [batch_size, n_step, n_class]\n",
    "    output = model(hidden, input_batch)\n",
    " \n",
    "    # output : [batch_size, n_class], target_batch : [batch_size, n_class)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_N2M5rSTcNj"
   },
   "source": [
    "#### 1.5 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9StYmSPjTP7N",
    "outputId": "1454837b-05bd-44f3-a958-f42626d05eaa"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "# Initial hidden state 0\n",
    "hidden = torch.tensor(torch.zeros(1, batch_size, n_hidden), device=device)\n",
    "\n",
    "print('Raw output of this model:\\n',model(hidden, input_batch))\n",
    "\n",
    "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29mr61tFVEmX"
   },
   "source": [
    "### Task 1: create the sevearl french words and the corresponding english words by yourself. Train a RNN model that can translate the french word into english word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSBZJqpRTiY7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjsQaSOkVk8i"
   },
   "source": [
    "### 2. Many-to-one LSTM (character level): Predict what is the next letter\n",
    "\n",
    "In this task, we will give our network to predict the final letter for us uisng LSTM. For example, if we key in 'lov' then the machine should give us 'e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo24zXN1d131"
   },
   "source": [
    "#### 2.1 Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1BkuKk-db_c"
   },
   "outputs": [],
   "source": [
    "# we need define all the possible letters\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "#word dictionary that can use to get the corresponding encoded number\n",
    "word_dict = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "# number dictionary that can be used to get the corresponding letter\n",
    "number_dict = {i: w for i, w in enumerate(char_arr)}\n",
    "\n",
    "n_class = len(word_dict) # number of class(=number of vocab)\n",
    "\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love', 'hate', 'live', 'home', 'hash', 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTNi04tpeCNJ"
   },
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]] # 'm', 'a' , 'k' is input\n",
    "        target = word_dict[seq[-1]] # 'e' is target\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return torch.tensor(input_batch, device=device), torch.tensor(target_batch, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyze2ep0eOWj"
   },
   "source": [
    "#### 2.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RV0SyyCeKaE"
   },
   "outputs": [],
   "source": [
    "# TextLSTM Parameters\n",
    "n_step = 3\n",
    "n_hidden = 128\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden)\n",
    "        self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
    "        self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
    "\n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0, 1)  # X : [n_step, batch_size, n_class]\n",
    "\n",
    "        hidden_state = torch.zeros(1, len(X), n_hidden)   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        cell_state = torch.zeros(1, len(X), n_hidden)     # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "\n",
    "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs[-1]  # [batch_size, n_hidden]\n",
    "        model = torch.mm(outputs, self.W) + self.b  # model : [batch_size, n_class]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoQyxb9RehCj"
   },
   "source": [
    "#### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9wn3ZP-eonH",
    "outputId": "81ebce9b-8633-4990-bd64-a5e8901ad5a3"
   },
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "model = TextLSTM().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "output = model(input_batch)\n",
    "\n",
    "# Training\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHlCD3aiez7d"
   },
   "source": [
    "#### 2.4 Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pj_ODzDxev9Q",
    "outputId": "2748de06-871f-48fc-830c-285c0b196213"
   },
   "outputs": [],
   "source": [
    "inputs = [sen[:3] for sen in seq_data]\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8df7rkUfXfR"
   },
   "source": [
    "### Task 2: \n",
    "\n",
    "1. Use whatever way you like, add more than 20 vocabulary and reuse the code to do the same task\n",
    "1. modify the model to make it predict one word each time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-ZJhmSfe5-X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaLAB5CV-biG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You should think about the problem when we have much bigger vocabulary that using dict to enumerate the words will make it very inefficient.\n",
    "\"Embedding\" and \"Tokenizer\" are the two soltuions available in [PyTorch](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmPW7hN5DhpL"
   },
   "outputs": [],
   "source": [
    "# an Embedding module containing 10 tensors of size 3: it is a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "embedding = nn.Embedding(10, 3)\n",
    "\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "print(input.shape)\n",
    "print(input)\n",
    "output = embedding(input)\n",
    "print(output.shape)  # output : [2, 4, 3] -> [batch_size, n_step, embedding_size]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced students practice part: Image captioning\n",
    "\n",
    "For who reach this part with plenty time left, you can try to play this level of code. In this exercise, you will implement vanilla RNN and use them to train a model that can generate novel captions for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import urllib.request, urllib.error, urllib.parse,tempfile\n",
    "from imageio import imread\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataset\n",
    "For this exercise, we will use the 2014 release of the [COCO dataset](https://cocodataset.org/), a standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "**Image features.** We have preprocessed the data and extracted features for you already. For all images, we have extracted features from the fc7 layer of the VGG-16 network pretrained on ImageNet, and these features are stored in the files `train2014_vgg16_fc7.h5` and `val2014_vgg16_fc7.h5`. To cut down on processing time and memory requirements, we have reduced the dimensionality of the features from 4096 to 512 using Principal Component Analysis (PCA), and these features are stored in the files `train2014_vgg16_fc7_pca.h5` and `val2014_vgg16_fc7_pca.h5`. The raw images take up nearly 20GB of space so we have not included them in the download. Since all images are taken from Flickr, we have stored the URLs of the training and validation images in the files `train2014_urls.txt` and `val2014_urls.txt`. This allows you to download images on-the-fly for visualization.\n",
    "\n",
    "**Captions.** Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is in the file `coco2014_vocab.json`, and you can use the function `decode_captions` to convert NumPy arrays of integer IDs back into strings.\n",
    "\n",
    "**Tokens.** There are a couple special tokens that we add to the vocabulary, and we have taken care of all implementation details around special tokens for you. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens.\n",
    "\n",
    "Download [coco_simple_files] by the following code\n",
    "and run the following cell to load all of the COCO data (captions, features, URLs, and vocabulary) using the `load_coco_data` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://cs231n.stanford.edu/coco_captioning.zip\"\n",
    "!unzip coco_captioning.zip\n",
    "!rm coco_captioning.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath('__file__'))  \n",
    "BASE_DIR = os.path.join(dir_path, \"coco_captioning\")\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "# We'll work with dimensionality-reduced features for the remainder of this assignment,\n",
    "# but you can also experiment with the original features on your own by changing the flag below.\n",
    "dir_path = os.path.dirname(os.path.realpath('__file__'))  \n",
    "BASE_DIR = os.path.join(dir_path, \"coco_captioning\")\n",
    "\n",
    "def load_coco_data(base_dir=BASE_DIR, max_train=None, pca_features=True):\n",
    "    print('base dir ', base_dir)\n",
    "    data = {}\n",
    "    caption_file = os.path.join(base_dir, \"coco2014_captions.h5\")\n",
    "    with h5py.File(caption_file, \"r\") as f:\n",
    "        for k, v in f.items():\n",
    "            data[k] = np.asarray(v)\n",
    "\n",
    "    if pca_features:\n",
    "        train_feat_file = os.path.join(base_dir, \"train2014_vgg16_fc7_pca.h5\")\n",
    "    else:\n",
    "        train_feat_file = os.path.join(base_dir, \"train2014_vgg16_fc7.h5\")\n",
    "    with h5py.File(train_feat_file, \"r\") as f:\n",
    "        data[\"train_features\"] = np.asarray(f[\"features\"])\n",
    "\n",
    "    if pca_features:\n",
    "        val_feat_file = os.path.join(base_dir, \"val2014_vgg16_fc7_pca.h5\")\n",
    "    else:\n",
    "        val_feat_file = os.path.join(base_dir, \"val2014_vgg16_fc7.h5\")\n",
    "    with h5py.File(val_feat_file, \"r\") as f:\n",
    "        data[\"val_features\"] = np.asarray(f[\"features\"])\n",
    "\n",
    "    dict_file = os.path.join(base_dir, \"coco2014_vocab.json\")\n",
    "    with open(dict_file, \"r\") as f:\n",
    "        dict_data = json.load(f)\n",
    "        for k, v in dict_data.items():\n",
    "            data[k] = v\n",
    "\n",
    "    train_url_file = os.path.join(base_dir, \"train2014_urls.txt\")\n",
    "    with open(train_url_file, \"r\") as f:\n",
    "        train_urls = np.asarray([line.strip() for line in f])\n",
    "    data[\"train_urls\"] = train_urls\n",
    "\n",
    "    val_url_file = os.path.join(base_dir, \"val2014_urls.txt\")\n",
    "    with open(val_url_file, \"r\") as f:\n",
    "        val_urls = np.asarray([line.strip() for line in f])\n",
    "    data[\"val_urls\"] = val_urls\n",
    "\n",
    "    # Maybe subsample the training data\n",
    "    if max_train is not None:\n",
    "        num_train = data[\"train_captions\"].shape[0]\n",
    "        mask = np.random.randint(num_train, size=max_train)\n",
    "        data[\"train_captions\"] = data[\"train_captions\"][mask]\n",
    "        data[\"train_image_idxs\"] = data[\"train_image_idxs\"][mask]\n",
    "#         data[\"train_features\"] = data[\"train_features\"][data[\"train_image_idxs\"]]\n",
    "    return data\n",
    "\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we sample a minibatch from this ```data``` and show the images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_captions(captions, idx_to_word):\n",
    "    singleton = False\n",
    "    if captions.ndim == 1:\n",
    "        singleton = True\n",
    "        captions = captions[None]\n",
    "    decoded = []\n",
    "    N, T = captions.shape\n",
    "    for i in range(N):\n",
    "        words = []\n",
    "        for t in range(T):\n",
    "            word = idx_to_word[captions[i, t]]\n",
    "            if word != \"<NULL>\":\n",
    "                words.append(word)\n",
    "            if word == \"<END>\":\n",
    "                break\n",
    "        decoded.append(\" \".join(words))\n",
    "    if singleton:\n",
    "        decoded = decoded[0]\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def sample_coco_minibatch(data, batch_size=100, split=\"train\"):\n",
    "    split_size = data[\"%s_captions\" % split].shape[0]\n",
    "    mask = np.random.choice(split_size, batch_size)\n",
    "    captions = data[\"%s_captions\" % split][mask]\n",
    "    image_idxs = data[\"%s_image_idxs\" % split][mask]\n",
    "    image_features = data[\"%s_features\" % split][image_idxs]\n",
    "    urls = data[\"%s_urls\" % split][image_idxs]\n",
    "    return captions, image_features, urls\n",
    "\n",
    "def image_from_url(url):\n",
    "    \"\"\"\n",
    "    Read an image from a URL. Returns a numpy array with the pixel data.\n",
    "    We write the image to a temporary file then read it back. Kinda gross.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = urllib.request.urlopen(url)\n",
    "        _, fname = tempfile.mkstemp()\n",
    "        with open(fname, \"wb\") as ff:\n",
    "            ff.write(f.read())\n",
    "        img = imread(fname)\n",
    "        os.remove(fname)\n",
    "        return img\n",
    "    except urllib.error.URLError as e:\n",
    "        print(\"URL Error: \", e.reason, url)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(\"HTTP Error: \", e.code, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "batch_size = 3\n",
    "\n",
    "captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "    plt.imshow(image_from_url(url))\n",
    "    plt.axis('off')\n",
    "    caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: design a RNN to caption these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNcjO9odI67yqCTVsVW6In",
   "collapsed_sections": [],
   "name": "Lab4_RNN_LSTM_language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
