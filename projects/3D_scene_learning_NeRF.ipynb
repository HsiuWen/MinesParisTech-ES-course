{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084c39e-0521-4d56-8b68-3b42d0b69d8d",
   "metadata": {},
   "source": [
    "# NeRF: representing scenes as neural radiance fields for view synthesis\n",
    "\n",
    "NeRF is a method that generates synthesizing novel views of complex scenes by optimizing a sparse set of input views. It is carried out by a fully connected deep network denoted as m, whose input is a single continuous 5D coordinate (location ($x,y,z$) and viewing direction ($\\theta, \\phi)$) and whose output is the volume density ($\\sigma$) and view-dependent emitted radiance ($RGB$) at that spatial location. \n",
    "$$\n",
    "[\\sigma,R,G,B]=m([x, y, z, \\theta, \\phi];\\Phi)\n",
    "$$\n",
    "\n",
    "![nerf](./figures/illustration_nerf.png)\n",
    "\n",
    "This neural radiance field represents a scene as the volumne density and directional emitted radiance at any point in space. Given the location and direction of the virtual camera, rendering a 2D image from this model requires estimating the integral $C(r)$ for a camear ray traced through each pixel. This integral consists of volume density $\\sigma$, transmittance $T(t)$ and particle on camera ray. \n",
    "$$\n",
    "    C(r)=\\int_{t_n}^{t_f}T(t)\\sigma(r(t))c(r(t),d)dt,\\quad where \\quad T(t)=exp(-\\int_{t_n}^{t_f}\\sigma(r(s))ds)\n",
    "$$\n",
    "In this exercise you will learn how to train a NeRF to render yourself in 3D from any angle of camera. Because it is a more difficult project that requires some knowledge of camera, this notebook provides the tutorial about how to generate camera parameters and data needed for training NeRF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f06e",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Have a partner take several photos of you standing steadily. Try to cover a wide range of angles and do not move!. Run the preprocessing code, so you generate the ground truth (camera parameters) of your photos\n",
    "2. Train and optimize MLP to predict the pixel with the given location of the camera and angle. This step is not trivial, and you should start with very small image resolutions to get a feeling for hyperparameters\n",
    "3. Visualize generated images. \n",
    "4. Use YOLO to detect the region of you in the image. And then modify the batch sampling based on the pixels within the bounding box. Experience the difference in accuracy and speed. \n",
    "\n",
    "**Important**: At the end, you should write a report of an adequate size, which will likely be at least half a page. In the report, you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g., \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task, and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended interestingly, describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e59f72",
   "metadata": {},
   "source": [
    "The packages you need to install before runnning this notebook (If you create your environment as instructed on my github, you already have everthing installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install torch\n",
    "!pip install ipykernel\n",
    "!pip install imageio\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777c6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import imageio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744323ec-6016-4ddd-84e1-cbb86c235a15",
   "metadata": {},
   "source": [
    "## Prerequist of learning NeRF\n",
    "\n",
    "### Camera model: \n",
    "\n",
    "A typical camera cmodel can be represented using a pinhole camera model, where the world scene is projected onto a 2D image plane through a pinhole at the center of the lens. The camera pose is defined by its position and orientation relative to the world coordinate system. As shown in the figure, $Y/Z=y/f$\n",
    "\n",
    "Given the world coordinate of object $[X,Y,Z]$ and its distance from the pinhole $Z$, the distance between the pinhole and the image plane $f$, the pixel coordinate can be derived by using the triangles argument. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\ y\\\\ z \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "fX/Z \\\\ fY/Z \\\\ f \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "f &  & & 0\\\\\n",
    " & f &  & 0\\\\\n",
    " &  & 1  & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}=diga(f,f,1)[I|0] \\bold{X}\n",
    "$$\n",
    "\n",
    "A principle point $(p_x,p_y)$ is the intersection location of the image plane with the principal axis. The above example assume this principle point is at (0,0). If it is not, the equation expressed in homoegenous coordinates are:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "fX \\\\ fY \\\\ Z \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "f &  & p_x & 0\\\\\n",
    " & f & p_y & 0\\\\\n",
    " &  & 1  & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}=K[I|0]\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05e87f",
   "metadata": {},
   "source": [
    "### camera rotation and translation\n",
    "![pinhole_model](figures//pinhole%20model%202.png)\n",
    "\n",
    "Say camera center in Cartesian coordinates is $\\tilde C$ and that camera rotation $R$. We can transform a Cartesian point $\\tilde{\\bold{X}}$ in the world coordinate system to a Cartesian point $\\bold{\\tilde{X}_{cam}}$ in the camera coordinates system as:\n",
    "$$\n",
    "\\bold{\\tilde{X}_{cam}}=\\bold{R}(\\tilde{\\bold{X}}-\\tilde{\\bold{C}}) =\n",
    "\\begin{bmatrix}\n",
    "\\bold{R} & -\\bold{R\\tilde{C}} \\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X \\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "$$\n",
    "Denote $\\bold{t}=-\\bold{R\\tilde{C}}$. We can see the final image coordinate is:\n",
    "$$\n",
    "x=\\bold{K[R|t]X}\n",
    "$$\n",
    "\n",
    "$\\bold{K}$ is intrinsics matrix which describe the internal parameters of the camera that relate the 3D world coordinates to 2D image coordinates. It constains focal length, principal point and pixel aspect ratio (1 if pixels are square)\n",
    "\n",
    "$\\bold{[R|t]}$ is the extrinsic matrix which desribes the transformation from world coordinates to camera coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd8e18",
   "metadata": {},
   "source": [
    "### Example from NeRF public dataset: lego\n",
    "\n",
    "Download example dataset from: https://www.kaggle.com/datasets/nguyenhung1903/nerf-synthetic-dataset\n",
    "\n",
    "unzip the file and choise one folder to learn how to load camera parameters and image. Change the name of ```base_dir``` according to your choice. \n",
    "\n",
    "**This sample code is used to help you undertand the theory of 3D scene reconstruction by images. So start with low resolution images (increase factor) to see the result first.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c448d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "base_dir = \"datasets/nerf_synthetic/lego\"\n",
    "# reduce the resolution by factor\n",
    "factor = 4\n",
    "# load camera intrinsic and extrinsic parameters from json file\n",
    "with open(os.path.join(base_dir, 'transforms_{}.json'.format('train')), 'r') as fp:\n",
    "    meta = json.load(fp)\n",
    "images = []\n",
    "cams = []\n",
    "for i in range(len(meta['frames'])):\n",
    "    frame = meta['frames'][0]\n",
    "    fname = os.path.join(base_dir, frame['file_path'] + '.png')\n",
    "    with open(fname, 'rb') as imgin:\n",
    "        image = np.array(Image.open(imgin), dtype=np.float32) / 255.\n",
    "        if factor >= 2:\n",
    "            [halfres_h, halfres_w] = [hw // factor for hw in image.shape[:2]]\n",
    "            image = cv2.resize(\n",
    "                image, (halfres_w, halfres_h), interpolation=cv2.INTER_AREA)\n",
    "    cams.append(np.array(frame['transform_matrix'], dtype=np.float32))\n",
    "    images.append(image)\n",
    "images = np.stack(np.array(images), axis=0)\n",
    "print('Number of images, width, height, channels:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb48bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0,:,:,:])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = images.shape[1:3]\n",
    "cam_to_world=cams[0]\n",
    "camera_angle_x=float(meta['camera_angle_x']) # angle of field of view in x direction\n",
    "focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
    "n_poses=images.shape[0]\n",
    "print('Focal length:', focal)\n",
    "print('Number of poses:', n_poses) \n",
    "print('Image height:', H)\n",
    "print('Image width:', W)\n",
    "print('Camera to world matrix ([R|t], extrinsic matrix) of first image:\\n', cam_to_world)\n",
    "print('Camera view of angle x (degree):', camera_angle_x*180/np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1e0d8",
   "metadata": {},
   "source": [
    "### Q1: How do we generate input and ground true for supervise learning?\n",
    "\n",
    "![overview_nerf](./figures/nerf_overview.png)\n",
    "\n",
    "Given set of images, algorithms such as structure-from-motion (COLMAP) can estimate camera poses, intrinsics and bouds. With these parameters, pixel RGB values its corresponding camera poses, view direction are stored as target y and input x, respectively.\n",
    "For example, an image (2x2) generate 4 training samples: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "xc & yc & zc & \\theta_1 & \\phi_1 & R1 & G1 & B1 \\\\\n",
    "xc & yc & zc & \\theta_2 & \\phi_2 & R2 & G2 & B2 \\\\\n",
    "xc & yc & zc & \\theta_3 & \\phi_3 & R3 & G3 & B3 \\\\\n",
    "xc & yc & zc & \\theta_4 & \\phi_4 & R4 & G4 & B4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since matrix multiplication is easier with 3D Cartesian unit vectors, the view direction is expressed as [d1,d2,d3]. So the size of the demonstrated dataset eventually is (pixel*pixel,9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2c487",
   "metadata": {},
   "source": [
    "### Genrate rays for NeRF training\n",
    " Since the ground true we have are the pixels from 2D images, we need to accumualte preditions on 3D space along the ray that pass the center of camera to the pixel. First, we calcaulte all pixel's world coordinates and the its direction from the camera center. Then we store the directions of pixels, origins of camera and viewdirs represented in world frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61176a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from [nerf_pl](https://github.com/kwea123/nerf_pl/blob/master/datasets/ray_utils.py#L27) github\n",
    "def get_rays(directions, c2w):\n",
    "    \"\"\"\n",
    "    Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
    "    Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
    "               ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
    "\n",
    "    Inputs:\n",
    "        directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
    "        c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
    "\n",
    "    Outputs:\n",
    "        rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
    "        rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
    "    \"\"\"\n",
    "    # Rotate ray directions from camera coordinate to the world coordinate\n",
    "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
    "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    # The origin of all rays is the camera origin in world coordinate\n",
    "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
    "\n",
    "    rays_d = rays_d.view(-1, 3)\n",
    "    rays_o = rays_o.view(-1, 3)\n",
    "\n",
    "    return rays_o, rays_d\n",
    "\n",
    "x, y = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),  # X-Axis (columns)\n",
    "        torch.arange(H, dtype=torch.float32),  # Y-Axis (rows)\n",
    "        indexing='ij')\n",
    "    \n",
    "    # ray directions for all pixels, same for all images (same H,W,focal)\n",
    "camera_directions = torch.stack(\n",
    "    [(x - W  * 0.5 + 0.5) / focal,\n",
    "    -(y - H  * 0.5 + 0.5) / focal,\n",
    "    -torch.ones_like(x)],\n",
    "    axis=-1) # (H, W, 3))\n",
    "\n",
    "rays_o, rays_d = get_rays(camera_directions, torch.from_numpy(cam_to_world[:3, :4]))\n",
    "print(rays_o.shape, rays_d.shape) # (H*W, 3) , (H*W, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97051e",
   "metadata": {},
   "source": [
    "Now we creeat a Dataset class like practical sessions for torch to load a batch of input during training, testing or validation. \n",
    "This class should have the following functions:\n",
    " - preprocess the raw images and read meta information\n",
    " - generate rays for each pixels.\n",
    " - return one input data that include image, rays, valid_mask (to tell if some of the pixels in the image should not be used), camera to world frame transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a6f08",
   "metadata": {},
   "source": [
    "Here are the functions used to package the preprocess in to a Dataset class for training process of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee5c15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlenderDataset(Dataset):\n",
    "    def __init__(self, root_dir,split='train',img_wh=(800,800)):\n",
    "        self.root_dir=root_dir\n",
    "        self.split=split\n",
    "        assert img_wh[0] == img_wh[1], 'image width must be equal to height'\n",
    "        self.img_wh=img_wh\n",
    "        self.define_transforms()\n",
    "\n",
    "        self.read_meta() # read camera parameters and images\n",
    "        self.white_back = True\n",
    "\n",
    "    def read_meta(self):\n",
    "        with open(os.path.join(self.root_dir, 'transforms_{}.json'.format(self.split)), 'r') as fp:\n",
    "            self.meta = json.load(fp)\n",
    "\n",
    "        w,h=self.img_wh\n",
    "        self.focal = .5 * w / np.tan(.5 * float(self.meta['camera_angle_x'])) #original focal length when w=800\n",
    "        self.focal *= self.img_wh[0]/800 #scale focal length to match the new resolution\n",
    "\n",
    "        #bounds, common for all scenes\n",
    "        self.near = 2.0\n",
    "        self.far = 6.0  \n",
    "        self.bounds = np.array([self.near, self.far], dtype=np.float32)\n",
    "        x, y = torch.meshgrid(\n",
    "            torch.arange(w, dtype=torch.float32),  # X-Axis (columns)\n",
    "            torch.arange(h, dtype=torch.float32),  # Y-Axis (rows)\n",
    "            indexing='ij')\n",
    "        \n",
    "        # ray directions for all pixels, same for all images (same H,W,focal)\n",
    "        self.directions = torch.stack(\n",
    "            [(x - w * 0.5 + 0.5) / self.focal,\n",
    "            -(y - h * 0.5 + 0.5) / self.focal,\n",
    "            -torch.ones_like(x)],\n",
    "            axis=-1) # (H, W, 3))\n",
    "        \n",
    "        if self.split =='train': #create buffer of all rays and rgb data\n",
    "            self.image_paths = []\n",
    "            self.poses = []\n",
    "            self.all_rays = []\n",
    "            self.all_rgbs = []\n",
    "\n",
    "            for frame in self.meta['frames']:\n",
    "                pose = np.array(frame['transform_matrix'], dtype=np.float32)[:3,:4]\n",
    "                self.poses += [pose]\n",
    "                c2w = torch.FloatTensor(pose)\n",
    "\n",
    "                image_path = os.path.join(self.root_dir, f\"{frame['file_path']}.png\")\n",
    "                self.image_paths += [image_path]\n",
    "                img = Image.open(image_path)\n",
    "                img = img.resize(self.img_wh, Image.LANCZOS)\n",
    "                img = self.transform(img) # (4, h, w)\n",
    "                img = img.view(4, -1).permute(1, 0) # (h*w, 4) RGBA\n",
    "                img = img[:, :3]*img[:, -1:] + (1-img[:, -1:]) # blend A to RGB\n",
    "                self.all_rgbs += [img]\n",
    "\n",
    "                rays_o, rays_d = get_rays(self.directions, c2w) # both (h*w, 3)\n",
    "\n",
    "                self.all_rays += [torch.cat([rays_o, rays_d, \n",
    "                                             self.near*torch.ones_like(rays_o[:, :1]),\n",
    "                                             self.far*torch.ones_like(rays_o[:, :1])],\n",
    "                                             1)] # (h*w, 8)\n",
    "        \n",
    "            self.all_rays = torch.cat(self.all_rays, 0) # (len(self.meta['frames])*h*w, 8)\n",
    "            self.all_rgbs = torch.cat(self.all_rgbs, 0) # (len(self.meta['frames])*h*w, 3)\n",
    "\n",
    "    def define_transforms(self):\n",
    "        self.transform = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == 'train':\n",
    "            return len(self.all_rays)\n",
    "        if self.split == 'val':\n",
    "            return 8 # only validate 8 images (to support <=8 gpus)\n",
    "        return len(self.meta['frames'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == 'train': # use data in the buffers\n",
    "            sample = {'rays': self.all_rays[idx],\n",
    "                      'rgbs': self.all_rgbs[idx]}\n",
    "\n",
    "        else: # create data for each image separately\n",
    "            frame = self.meta['frames'][idx]\n",
    "            c2w = torch.FloatTensor(frame['transform_matrix'])[:3, :4]\n",
    "\n",
    "            img = Image.open(os.path.join(self.root_dir, f\"{frame['file_path']}.png\"))\n",
    "            img = img.resize(self.img_wh, Image.LANCZOS)\n",
    "            img = self.transform(img) # (4, H, W)\n",
    "            valid_mask = (img[-1]>0).flatten() # (H*W) valid color area\n",
    "            img = img.view(4, -1).permute(1, 0) # (H*W, 4) RGBA\n",
    "            img = img[:, :3]*img[:, -1:] + (1-img[:, -1:]) # blend A to RGB\n",
    "\n",
    "            rays_o, rays_d = get_rays(self.directions, c2w)\n",
    "\n",
    "            rays = torch.cat([rays_o, rays_d, \n",
    "                              self.near*torch.ones_like(rays_o[:, :1]),\n",
    "                              self.far*torch.ones_like(rays_o[:, :1])],\n",
    "                              1) # (H*W, 8)\n",
    "\n",
    "            sample = {'rays': rays,\n",
    "                      'rgbs': img,\n",
    "                      'c2w': c2w,\n",
    "                      'valid_mask': valid_mask}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = BlenderDataset(root_dir=base_dir, split='train', img_wh=(400,400))\n",
    "\n",
    "data_loader = DataLoader(mydataset, batch_size=1024, shuffle=True)\n",
    "samples_ = next(iter(data_loader))\n",
    "print(samples_['rays'].shape)# (Batch, [origin, direction, near, far])\n",
    "print(samples_['rgbs'].shape)# (Batch, [red, green, blue]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381373cf",
   "metadata": {},
   "source": [
    "### Q2: How to render (predict) a new image with given position and view direction of camera?\n",
    "The output of model is not directly the color of each pixel, it is the density and color at 3D spatial position. To project 3D radiant onto a 2D image, rendering algorithm is applied. As illustrated in figure 2, discretilization is needed to numerically estiamte equation 1. This invloves queried several points on the ray and limit the speed of rendering. We can see better methods to do volume rendering. Here we desmonstrate the original one proposed by the paper: partition $[t_n, t_f]$ into N evenly spaced bins and then draw one sample $t_i$ uniformly at random from within each bin.\n",
    "\n",
    "$$\n",
    "t_i \\sim u\\left[t_n + \\frac{i-1}{N}(t_f - t_n),\\;\\; t_n + \\frac{i}{N}(t_f - t_n)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2838d-8549-43cd-a6cc-5d42ea0272fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn=0\n",
    "hf=0.5\n",
    "nb_bins=10\n",
    "\n",
    "ray_origins = samples_['rays'][0, :3] # get the origin of the first ray\n",
    "ray_directions = samples_['rays'][0, 3:6] # get the direction of the first ray\n",
    "\n",
    "t = torch.linspace(hn, hf, nb_bins).expand(ray_origins.shape[0], nb_bins)\n",
    "# Perturb sampling along each ray.\n",
    "mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "lower = torch.cat((t[:, :1], mid), -1)\n",
    "upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "u = torch.rand(t.shape)\n",
    "t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10],).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "x = ray_origins.unsqueeze(1) + t * ray_directions.unsqueeze(1)  \n",
    "print(f\"Sampled 10 3D points along the first ray of first image:\\n {x.transpose(1,0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f106281",
   "metadata": {},
   "source": [
    "Estimating C(r) with quadrature rule is given as:\n",
    "\n",
    "$$\n",
    "\\hat{C}(r)=\\sum_{i=1}^{N}T_i(1-\\exp{(-\\sigma_i \\delta_i)})c_i \\\\\n",
    "T_i=\\exp{(-\\sum_{j=1}^{i-1}\\sigma_j \\delta_j)},\n",
    "$$\n",
    "\n",
    "where $\\delta_i=t_{i+1}-t_i$ is the distance between adjacent samples.\n",
    "\n",
    "This function can reduces to traditional alpha compositing with alpha values:\n",
    "$$\n",
    "\\alpha_i=1-\\exp{(-\\sigma_i \\delta_i)} \\\\\n",
    "\\hat{C}(r)=\\sum_{i=1}^{N} T_i \\alpha_i c_i \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9aa43f5-5167-44e1-bc25-55eaa24a4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    # Perturb sampling along each ray.\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor(\n",
    "        [1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    # Compute the 3D points along each ray\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)\n",
    "    # Expand the ray_directions tensor to match the shape of x\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    alpha = 1 - torch.exp(-sigma.reshape(x.shape[:-1]) * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights * colors.reshape(x.shape)).sum(dim=1)\n",
    "    weight_sum = weights.sum(-1).sum(-1)  # Regularization for white background\n",
    "    return c + 1 - weight_sum.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceef093",
   "metadata": {},
   "source": [
    "Positioning encoding is applied to 5D input in order to represent high-frequency variation in color and geometry. \n",
    "The authors designed separate MLPs to predict density and emitted color. The first MLP predicts the volume density as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction. Now we pack all these functions into NerfModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3899c3e8-9f3e-4c1e-af87-a005b781eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):   \n",
    "        super(NerfModel, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "        # density estimation\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
    "        # color estimation\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos) # emb_x: [batch_size, embedding_dim_pos * 6]\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction) # emb_d: [batch_size, embedding_dim_direction * 6]\n",
    "        h = self.block1(emb_x) # h: [batch_size, hidden_dim]\n",
    "        tmp = self.block2(torch.cat((h, emb_x), dim=1)) # tmp: [batch_size, hidden_dim + 1]\n",
    "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1]) # h: [batch_size, hidden_dim], sigma: [batch_size]\n",
    "        h = self.block3(torch.cat((h, emb_d), dim=1)) # h: [batch_size, hidden_dim // 2]\n",
    "        c = self.block4(h) # c: [batch_size, 3]\n",
    "        return c, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b069e-b141-414b-8fea-9fbfc4ff9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# TODO Training Loop\n",
    "mymodel= NerfModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = mymodel.to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "# Define loss function\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "training_loss = []\n",
    "for _ in tqdm(range(num_epochs)):\n",
    "    # Randomly sample batch_size rays from the training dataset\n",
    "    for batch_data in data_loader:\n",
    "        batch_data = next(iter(data_loader))\n",
    "        batch_rays = batch_data['rays'].to(device)  # (batch_size, 8)\n",
    "        batch_rgb = batch_data['rgbs'].to(device)  # (batch_size, 3)\n",
    "\n",
    "        ray_origins = batch_rays[:, :3] # (batch_size, 3)\n",
    "        ray_directions = batch_rays[:, 3:6] # (batch_size, 3)\n",
    "      \n",
    "        # Forward pass: render the rays using the NeRF model\n",
    "        pred_rgb = render_rays(model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(pred_rgb, batch_rgb)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "    print(f\"Epoch {_+1}/{num_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Save model after training\n",
    "if not os.path.exists('../models'):\n",
    "    os.makedirs('../models')\n",
    "torch.save(model.state_dict(), '../models/nerf_model.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c759df5",
   "metadata": {},
   "source": [
    "### Test your model \n",
    "\n",
    "To understand the use of NeRF in rendering scene, here we load a trained NeRF model and run the test images for demonstration. You can not run this part if you don't have a save model named \"nerf_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7157a8c-274a-4cee-b4b4-1c6720dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load model to estimate density and color at 3D spatial position\n",
    "model = NerfModel(hidden_dim=256).to(device)\n",
    "model.load_state_dict(torch.load('../models/nerf_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "mytestingdataset = BlenderDataset(root_dir=base_dir, split='test', img_wh=(400,400))\n",
    "print('Number of rays in the testing dataset:', len(mytestingdataset))\n",
    "\n",
    "hn,hf=2,6\n",
    "chunk_size=20 # process 20 rays at a time to avoid OOM\n",
    "for img_index in range(2):\n",
    "    ray_origins = mytestingdataset[img_index]['rays'][:, :3]\n",
    "    ray_directions = mytestingdataset[img_index]['rays'][:, 3:6]\n",
    "\n",
    "    px_values = []   # list of regenerated pixel values\n",
    "    for i in range(int(np.ceil(H / chunk_size))):   # iterate over chunks\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "        px_values.append(render_rays(model, ray_origins_, ray_directions_,\n",
    "                                     hn=hn, hf=hf, nb_bins=nb_bins))\n",
    "    img = torch.cat(px_values).data.cpu().numpy().reshape(H, W, 3)\n",
    "    img = (img.clip(0, 1)*255.).astype(np.uint8)\n",
    "    img_rendered = Image.fromarray(img)\n",
    "\n",
    "\n",
    "    # Ground truth image from testing_dataset\n",
    "    img_gt = mytestingdataset[img_index]['rgbs'] \n",
    "    img_gt = img_gt.reshape(H, W, 3)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(img_rendered)\n",
    "    axes[0].set_title(\"Rendered Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(img_gt)\n",
    "    axes[1].set_title(\"Ground Truth Image\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e1e59-ace4-4552-b05c-e155a4552c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Visualization\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - After training, visualize some generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714b242",
   "metadata": {},
   "source": [
    "### Run your own data steps:\n",
    "- Install [COLMAP](https://github.com/colmap/colmap) following [installation guide](https://colmap.github.io/install.html)\n",
    "- Prepare your images in a folder (around 20 to 30 for forward-facing, and 40 to 50 for 360 inward-facing) and run COLMAP to get images and camera poses. Be aware that the process can fail easily due to several reasons. To increase your success rate, make sure you have around 20 to 30 for forward-facing, and 40 to 50 for 360 inward-facing.\n",
    "- Clone [LLFF] and run ```python img2poses.py $your-images-folder```. This code helps you read output files from COLMAP and generate poses_bounds.npy file that contains the information you need for NeRF training. \n",
    "\n",
    "After running colmap, you will get a poses_bounds.npy file under your data folder. Once you get that, you're ready to train!\n",
    "Common issues:\n",
    "\n",
    "\n",
    "Here are two ways to run COLMAP:\n",
    "\n",
    "(a) GUI interface: if your system is Windows, download these pre-built binaries (https://github.com/colmap/colmap/releases/tag/3.13.0). Run ./colmap.bat in your terminal, and then you can create a new project. Run feature extraction. GUI gives you a fast way to know the advanced settings of camera models, and you can see the result of the reconstructed scene in this GUI, too. \n",
    "\n",
    "(b) See the next cell to run colmap in terminal style. \n",
    "\n",
    "*** Make sure your images are saved in a folder called \"images\" under your major base_dir. Two ways to estimate camera poses are provided here.\n",
    "\n",
    "*** HEIC images: the format is a compressed format and is not supported. Conversion can be found [here](https://github.com/dragonGR/PyHEIC2JPG) or change the camera setting of your iPhone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install conda-forge::colmap\n",
    "%git clone https://github.com/Fyusion/LLFF\n",
    "%cd /content/LLFF\n",
    "%pip install -r requirements.txt\n",
    "%python imgs2poses.py '../../MinesParisTech-ES-course/projects/datasets/Table' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed925a5f",
   "metadata": {},
   "source": [
    "If your COLPAM features extraction is not failed, you should be able to find a file called ```pose_bounds.npy```.  This file contains 3x5 pose matrices and 2 depth bounds (near, far) for each image. Each pose has [R T] as the left 3x4 matrix and [H W F] as the right 3x1 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../MinesParisTech-ES-course/projects/datasets/cookies_bag'\n",
    "poses_arr = np.load(os.path.join(base_dir, 'poses_bounds.npy')) \n",
    "poses = poses_arr[:, :-2].reshape([-1, 3, 5]).transpose([1,2,0]) # reshape and transpose to get poses of shape (3,5,N)\n",
    "bds = poses_arr[:, -2:].transpose([1,0]) # get bounds of shape (2,N)\n",
    "print('poses shape:', poses.shape)\n",
    "print('bounds shape:', bds.shape)\n",
    "print('First pose:\\n', poses[:,:,0])\n",
    "print('First bound:\\n', bds[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15cb22",
   "metadata": {},
   "source": [
    "You should create a Dataset class for your own images loading. Then you should be able to resue the functions provided before. The additional bonus will be given if you can implement other techniques that make your training faster or more accuracy, such as using pretrained YOLO to detect the pixels that contains the target 3D object your model aims to learn. By applying mask we can reduce the valid training samples and make sure NeRF is learning the targeting object. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551daa08",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
