{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084c39e-0521-4d56-8b68-3b42d0b69d8d",
   "metadata": {},
   "source": [
    "# NeRF: representing scenes as neural radiance fields for view synthesis\n",
    "\n",
    "NeRF is a method that generates synthesizing novel views of complex scenes by optimizing a sparse set of input views. It is carried out by a fully connected deep network denoted as m, whose input is a single continuous 5D coordinate (location ($x,y,z$) and viewing direction ($\\theta, \\phi)$) and whose output is the volume density ($\\sigma$) and view-dependent emitted radiance ($RGB$) at that spatial location. \n",
    "$$\n",
    "[\\sigma,R,G,B]=m([x, y, z, \\theta, \\phi];\\Phi)\n",
    "$$\n",
    "\n",
    "![nerf](./figures/illustration_nerf.png)\n",
    "\n",
    "This neural radiance field represents a scene as the volumne density and directional emitted radiance at any point in space. Given the location and direction of the virtual camera, rendering a 2D image from this model requires estimating the integral $C(r)$ for a camear ray traced through each pixel. This integral consists of volume density $\\sigma$, transmittance $T(t)$ and particle on camera ray. \n",
    "$$\n",
    "    C(r)=\\int_{t_n}^{t_f}T(t)\\sigma(r(t))c(r(t),d)dt,\\quad where \\quad T(t)=exp(-\\int_{t_n}^{t_f}\\sigma(r(s))ds)\n",
    "$$\n",
    "In this exercise you will try to learn a 3D object of yourself. Because this is one of the more difficult exercises a maximum amount of code is already given. However, you still need to add some code and do training and evalluation.\n",
    "\n",
    "## Tasks\n",
    "1. Have your partner take several photos of you. Try to cover wide range of angle and don't move!. Run the preprocessing code so you generate the grouth truth (camera parameters) of your photos\n",
    "2. Train and optimize your . This step is not trivial and you should start with very small image resolutions to get a feeling for hyperparameters\n",
    "3. Visualize generated images. \n",
    "4. Use YOLO to detect the region of you on the image. And then modify the batch sampling by the pixels locate in the bounding box. Experience the difference in accuracy and speed. \n",
    " \n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e59f72",
   "metadata": {},
   "source": [
    "The packages you need for creating virtual environment for this project:\n",
    "\n",
    " - python==3.13\n",
    " - numpy==2.3.4\n",
    " - pillow==12.0.0\n",
    " - torch==2.9.0\n",
    " - tqdm==4.67.1\n",
    " - imageio==2.37.0\n",
    " - opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8660c",
   "metadata": {},
   "source": [
    "If you have error to install opencv-python, use the way to install\n",
    "\n",
    "```\n",
    "git clone https://github.com/opencv/opencv-python.git\n",
    "cd opencv-python\n",
    "pip install scikit-build setuptools wheel cmake\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c6454",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744323ec-6016-4ddd-84e1-cbb86c235a15",
   "metadata": {},
   "source": [
    "## Prerequist of learning NeRF\n",
    "\n",
    "### Camera model: \n",
    "\n",
    "A typical camera cmodel can be represented using a pinhole camera model, where the world scene is projected onto a 2D image plane through a pinhole at the center of the lens. The camera pose is defined by its position and orientation relative to the world coordinate system. As shown in the figure, $Y/Z=y/f$\n",
    "\n",
    "Given the world coordinate of object $[X,Y,Z]$ and its distance from the pinhole $Z$, the distance between the pinhole and the image plane $f$, the pixel coordinate can be derived by using the triangles argument. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x \\\\ y\\\\ z \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "fX/Z \\\\ fY/Z \\\\ f \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "f &  & & 0\\\\\n",
    " & f &  & 0\\\\\n",
    " &  & 1  & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}=diga(f,f,1)[I|0] \\bold{X}\n",
    "$$\n",
    "\n",
    "A principle point $(p_x,p_y)$ is the intersection location of the image plane with the principal axis. The above example assume this principle point is at (0,0). If it is not, the equation expressed in homoegenous coordinates are:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "fX \\\\ fY \\\\ Z \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "f &  & p_x & 0\\\\\n",
    " & f & p_y & 0\\\\\n",
    " &  & 1  & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}=K[I|0]\n",
    "\\begin{bmatrix}\n",
    "X \\\\ Y \\\\ Z \\\\ 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05e87f",
   "metadata": {},
   "source": [
    "### camera rotation and translation\n",
    "![pinhole_model](figures//pinhole%20model%202.png)\n",
    "\n",
    "Say camera center in Cartesian coordinates is $\\tilde C$ and that camera rotation $R$. We can transform a Cartesian point $\\tilde{\\bold{X}}$ in the world coordinate system to a Cartesian point $\\bold{\\tilde{X}_{cam}}$ in the camera coordinates system as:\n",
    "$$\n",
    "\\bold{\\tilde{X}_{cam}}=\\bold{R}(\\tilde{\\bold{X}}-\\tilde{\\bold{C}}) =\n",
    "\\begin{bmatrix}\n",
    "\\bold{R} & -\\bold{R\\tilde{C}} \\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}X \\\\Y\\\\Z\\\\1\\end{bmatrix}\n",
    "$$\n",
    "Denote $\\bold{t}=-\\bold{R\\tilde{C}}$. We can see the final image coordinate is:\n",
    "$$\n",
    "x=\\bold{K[R|t]X}\n",
    "$$\n",
    "\n",
    "$\\bold{K}$ is intrinsics matrix which describe the internal parameters of the camera that relate the 3D world coordinates to 2D image coordinates. It constains focal length, principal point and pixel aspect ratio (1 if pixels are square)\n",
    "\n",
    "$\\bold{[R|t]}$ is the extrinsic matrix which desribes the transformation from world coordinates to camera coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd8e18",
   "metadata": {},
   "source": [
    "### Example from NeRF public dataset: lego\n",
    "\n",
    "You can search nerf_synthetic.zip file on internet and unzip to your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c448d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# data path\n",
    "base_dir = \"../datasets/nerf_synthetic/lego\"\n",
    "# reduce the resolution by factor\n",
    "factor = 4\n",
    "# load camera intrinsic and extrinsic parameters from json file\n",
    "with open(os.path.join(base_dir, 'transforms_{}.json'.format('train')), 'r') as fp:\n",
    "    meta = json.load(fp)\n",
    "images = []\n",
    "cams = []\n",
    "for i in range(len(meta['frames'])):\n",
    "    frame = meta['frames'][0]\n",
    "    fname = os.path.join(base_dir, frame['file_path'] + '.png')\n",
    "    with open(fname, 'rb') as imgin:\n",
    "        image = np.array(Image.open(imgin), dtype=np.float32) / 255.\n",
    "        if factor >= 2:\n",
    "            [halfres_h, halfres_w] = [hw // 2 for hw in image.shape[:2]]\n",
    "            image = cv2.resize(\n",
    "                image, (halfres_w, halfres_h), interpolation=cv2.INTER_AREA)\n",
    "    cams.append(np.array(frame['transform_matrix'], dtype=np.float32))\n",
    "    images.append(image)\n",
    "images = np.stack(np.array(images), axis=0)\n",
    "print('Number of images, width, height, channels:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb48bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0,:,:,:])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = images.shape[1:3]\n",
    "cam_to_world=cams[0]\n",
    "camera_angle_x=float(meta['camera_angle_x']) # angle of field of view in x direction\n",
    "focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
    "n_poses=images.shape[0]\n",
    "print('Focal length:', focal)\n",
    "print('Number of poses:', n_poses) \n",
    "print('Image height:', H)\n",
    "print('Image width:', W)\n",
    "print('Camera to world matrix ([R|t], extrinsic matrix) of first image:\\n', cam_to_world)\n",
    "print('Camera view of angle x (degree):', camera_angle_x*180/np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2c487",
   "metadata": {},
   "source": [
    "### Genrate rays for NeRF training\n",
    "First, we calcaulte all pixel's world coordinates and the its direction from the camera center.\n",
    "\n",
    "Optionally, we can calculate the distance from each unit-norm direction vector to its x-axis neighbor which is used for MipNeRF as Radiance for conical frustum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61176a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "near= 2.0 # object in the scene is between near and far plane\n",
    "far= 6.0\n",
    "x, y = np.meshgrid(\n",
    "    np.arange(W, dtype=np.float32),  # X-Axis (columns)\n",
    "    np.arange(H, dtype=np.float32),  # Y-Axis (rows)\n",
    "    indexing='xy')\n",
    "\n",
    "camera_directions = np.stack(\n",
    "    [(x - W * 0.5 + 0.5) / focal,\n",
    "    -(y - H * 0.5 + 0.5) / focal,\n",
    "    -np.ones_like(x)],\n",
    "    axis=-1) # 400x400x3\n",
    "\n",
    "# Rotate ray directions from camera frame to the world frame\n",
    "directions = ((camera_directions[..., None, :] * cam_to_world[None, None, :3, :3]).sum(axis=-1))  # Translate camera frame's origin to the world frame\n",
    "origins = np.broadcast_to(cam_to_world[None, None, :3, -1], directions.shape)\n",
    "viewdirs = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "print(origins.shape, directions.shape, viewdirs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1e0d8",
   "metadata": {},
   "source": [
    "### Q1: How do we generate input and ground true for supervise learning?\n",
    "\n",
    "![overview_nerf](./figures/nerf_overview.png)\n",
    "\n",
    "Given set of images, algorithms such as structure-from-motion (COLMAP) can estimate camera poses, intrinsics and bouds. With these parameters, pixel RGB values its corresponding camera poses, view direction are stored as target y and input x, respectively.\n",
    "For example, an image (2x2) generate 4 training samples: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "xc & yc & zc & \\theta_1 & \\phi_1 & R1 & G1 & B1 \\\\\n",
    "xc & yc & zc & \\theta_2 & \\phi_2 & R2 & G2 & B2 \\\\\n",
    "xc & yc & zc & \\theta_3 & \\phi_3 & R3 & G3 & B3 \\\\\n",
    "xc & yc & zc & \\theta_4 & \\phi_4 & R4 & G4 & B4 \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381373cf",
   "metadata": {},
   "source": [
    "### Q2: How to render (predict) a new image with given position and view direction of camera?\n",
    "The output of model is not directly the color of each pixel, it is the density and color at 3D spatial position. To project 3D radiant onto a 2D image, rendering algorithm is applied. As illustrated in figure 2, discretilization is needed to numerically estiamte equation 1. This invloves queried several points on the ray and limit the speed of rendering. We can see better methods to do volume rendering. Here we desmonstrate the original one proposed by the paper: partition $[t_n, t_f]$ into N evenly spaced bins and then draw one sample $t_i$ uniformly at random from within each bin.\n",
    "\n",
    "$$\n",
    "t_i \\sim u\\left[t_n + \\frac{i-1}{N}(t_f - t_n),\\;\\; t_n + \\frac{i}{N}(t_f - t_n)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2838d-8549-43cd-a6cc-5d42ea0272fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn=0\n",
    "hf=0.5\n",
    "nb_bins=10\n",
    "\n",
    "ray_origins = training_dataset[img_index * H * W: (img_index + 1) * H * W, :3][0]\n",
    "ray_directions = training_dataset[img_index * H * W: (img_index + 1) * H * W, 3:6][0]\n",
    "\n",
    "t = torch.linspace(hn, hf, nb_bins).expand(ray_origins.shape[0], nb_bins)\n",
    "# Perturb sampling along each ray.\n",
    "mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "lower = torch.cat((t[:, :1], mid), -1)\n",
    "upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "u = torch.rand(t.shape)\n",
    "t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10],).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "x = ray_origins.unsqueeze(1) + t * ray_directions.unsqueeze(1)  \n",
    "print(f\"Sampled 3D points along a ray:\\n {x.transpose(1,0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f106281",
   "metadata": {},
   "source": [
    "Estimating C(r) with quadrature rule is given as:\n",
    "\n",
    "$$\n",
    "\\hat{C}(r)=\\sum_{i=1}^{N}T_i(1-\\exp{(-\\sigma_i \\delta_i)})c_i \\\\\n",
    "T_i=\\exp{(-\\sum_{j=1}^{i-1}\\sigma_j \\delta_j)},\n",
    "$$\n",
    "\n",
    "where $\\delta_i=t_{i+1}-t_i$ is the distance between adjacent samples\n",
    "This function can reduces to traidtional alpha compositing with alpha values:\n",
    "$$\n",
    "\\alpha_i=1-\\exp{(-\\sigma_i \\delta_i)} \\\\\n",
    "\\hat{C}(r)=\\sum_{i=1}^{N} T_i \\alpha_i c_i \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "L = 16\n",
    "F = 2\n",
    "T = 2**19\n",
    "N_min = 16\n",
    "N_max = 2048\n",
    "b = np.exp((np.log(N_max) - np.log(N_min)) / (L - 1))\n",
    "Nl = [int(np.floor(N_min * b**l)) for l in range(L)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa43f5-5167-44e1-bc25-55eaa24a4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    # Perturb sampling along each ray.\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor(\n",
    "        [1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    # Compute the 3D points along each ray\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)\n",
    "    # Expand the ray_directions tensor to match the shape of x\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    alpha = 1 - torch.exp(-sigma.reshape(x.shape[:-1]) * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    c = (weights * colors.reshape(x.shape)).sum(dim=1)\n",
    "    weight_sum = weights.sum(-1).sum(-1)  # Regularization for white background\n",
    "    return c + 1 - weight_sum.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceef093",
   "metadata": {},
   "source": [
    "Positioning encoding is applied to 5D input in order to represent high-frequency variation in color and geometry. \n",
    "The authors designed separate MLPs to predict density and emitted color. The first MLP predicts the volume density as a function of only the location x, while allowing the RGB color c to be predicted as a function of both location and viewing direction. Now we pack all these functions into NerfModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899c3e8-9f3e-4c1e-af87-a005b781eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):   \n",
    "        super(NerfModel, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "        # density estimation\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
    "        # color estimation\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos) # emb_x: [batch_size, embedding_dim_pos * 6]\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction) # emb_d: [batch_size, embedding_dim_direction * 6]\n",
    "        h = self.block1(emb_x) # h: [batch_size, hidden_dim]\n",
    "        tmp = self.block2(torch.cat((h, emb_x), dim=1)) # tmp: [batch_size, hidden_dim + 1]\n",
    "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1]) # h: [batch_size, hidden_dim], sigma: [batch_size]\n",
    "        h = self.block3(torch.cat((h, emb_d), dim=1)) # h: [batch_size, hidden_dim // 2]\n",
    "        c = self.block4(h) # c: [batch_size, 3]\n",
    "        return c, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b069e-b141-414b-8fea-9fbfc4ff9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c759df5",
   "metadata": {},
   "source": [
    "### Test your model \n",
    "\n",
    "To understand the use of NeRF in rendering scene, here we load a trained NeRF model and run the test images for demonstration. You can not run this part if you don't have a save model named \"nerf_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7157a8c-274a-4cee-b4b4-1c6720dcc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load model to estimate density and color at 3D spatial position\n",
    "model = NerfModel(hidden_dim=256).to(device)\n",
    "model.load_state_dict(torch.load('../models/nerf_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "hn,hf=2,6\n",
    "chunk_size=20\n",
    "for img_index in range(2):\n",
    "    ray_origins = testing_dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
    "    ray_directions = testing_dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
    "\n",
    "    px_values = []   # list of regenerated pixel values\n",
    "    for i in range(int(np.ceil(H / chunk_size))):   # iterate over chunks\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
    "        px_values.append(render_rays(model, ray_origins_, ray_directions_,\n",
    "                                     hn=hn, hf=hf, nb_bins=nb_bins))\n",
    "    img = torch.cat(px_values).data.cpu().numpy().reshape(H, W, 3)\n",
    "    img = (img.clip(0, 1)*255.).astype(np.uint8)\n",
    "    img_rendered = Image.fromarray(img)\n",
    "\n",
    "\n",
    "    # Ground truth image from testing_dataset\n",
    "    img_gt = testing_dataset[img_index * H * W : (img_index + 1) * H * W, 6:]\n",
    "    img_gt = img_gt.reshape(H, W, 3)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(img_rendered)\n",
    "    axes[0].set_title(\"Rendered Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(img_gt)\n",
    "    axes[1].set_title(\"Ground Truth Image\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e1e59-ace4-4552-b05c-e155a4552c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Visualization\n",
    "# -------------------\n",
    "# Instructions:\n",
    "# - After training, visualize some generated images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
